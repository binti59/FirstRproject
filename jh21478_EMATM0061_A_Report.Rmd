---
title: "jh21478_EMATM0061_A_Report"
author: "Bikramjit Chowdhury"
date: "29/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
##install.packages("tidyverse")
library(tidyverse)
##install.packages("Stat2Data")
library(Stat2Data)
library(readxl) #load the readxl library
library(ggplot2)
library(glmnet)
library(stringr)

#Section A
#Locate the folder path to read the file
folder_path<-"/Users/bikramjitchowdhury/Downloads/SCEM/jh21478_EMATM0061_summative_assessment/jh21478_EMATM0061_A/"
#Define the file name
file_name<-"finance_data_EMATM0061.csv" #
#Define the file path
file_path<-paste(folder_path,file_name,sep="") # create the file_path
#Read the file and assign it to a new data frame finance_data_original 

finance_data_original<-read_csv(file_path) 
#Answer A.1
#Rows: 1051 Columns: 30 -There are 1051 rows and 30 columns in this file

#Answer A.2
#Calculate the number of columns in the original data frame
ncol(finance_data_original) # 30

#Calculate the number of rows in the original data frame
nrow(finance_data_original)  #1051

#Create a subset of records called finance_data with 5 rename columns but 1051 rows
finance_data<-finance_data_original%>%select(state_year_code,Details.Education.Education.Total,Details.Health.Health.Total.Expenditure,Details.Transportation.Highways.Highways.Total.Expenditure,Totals.Revenue,Totals.Expenditure)%>%rename(education_expenditure=Details.Education.Education.Total,health_expenditure=Details.Health.Health.Total.Expenditure,transport_expenditure=Details.Transportation.Highways.Highways.Total.Expenditure,totals_revenue=Totals.Revenue,totals_expenditure=Totals.Expenditure) 


#Display the results for the first 5 rows and first 5 columns only
head(finance_data%>%select(state_year_code,education_expenditure,health_expenditure),5)

#Answer A.3
#Add the new column  total_savings in the finance data dataframe
finance_data<-finance_data%>%mutate(totals_savings=(totals_revenue-totals_expenditure))

#Display the first 3 rows and the four columns “state_year_code”,“totals_revenue”,“totals_expenditure”,“totals_savings” of the data frame finance_data
head(finance_data%>%select(state_year_code,totals_revenue,totals_expenditure,totals_savings),3)

#A.4


#Separate the state_year_code coloumn into two seperate columns
finance_data<-finance_data%>%separate(state_year_code,into=c("state","year"),sep="__")

#Convert the states so that they appear with the first letter of each word in upper case and the remainder in lower case

finance_data$state=str_to_title(finance_data$state)

#Display the first 3 rows and the four columns
head(finance_data%>%select(state,year,totals_revenue,totals_expenditure,totals_savings),3)

#A.5
#Generate a plot which displays the total revenue (“total_revenue”) as function of the year (“year”) for the following
#four states: Louisiana, Montana, Mississippi and Kentucky.
#Display the revenue in terms of millions of dollars

finance_data%>%rename(State=state)%>%filter(State %in% c("Louisiana","Montana","Mississippi","Kentucky"))%>%ggplot(aes(x=as.numeric(year),y=totals_revenue/1000000),color=State)+xlab("Year")+theme_bw()+ylab("Revenue(in millions)")+geom_smooth(aes(color=State,linetype=State),size=1)

#A.6

#Create a function called get_decade() which takes as input a number and rounds that number down to the
#nearest multiple of 10. 

get_decade<-function(x){
 
  y=floor((x/10))*10  ;
  return(y);
}

get_decade(243)

#Use your get_decade() function to add a new column to the “finance_data” data frame called “decade” which should give the decade corresponding to the year column.

finance_data<-finance_data%>%mutate(decade=get_decade(as.numeric(year)))

#Three states had the highest mean-average savings (“totals_savings”) over the decade starting 2000

finance_data%>%filter(decade==2000)%>%group_by(state)%>%summarise(mean_total_savings=mean(totals_savings,na.rm=TRUE))%>%arrange(desc(mean_total_savings))%>%head(3)

#The three states with the highest mean total savings are Texas, Ohio and California.

# Create the Alaska summary data frame with the following properties
#(a) “decade” – the decade (1990, 2000, 2010)
#(b) “ed_mn” – the mean of the education expenditure in Alaska for the corresponding decade
#(c) “ed_md” – the median of the education expenditure in Alaska for the corresponding decade
#(d) “he_mn” – the mean of the health expenditure in Alaska for the corresponding decade
#(e) “he_md” – the median of the health expenditure in Alaska for the corresponding decade
#(f) “tr_mn” – the mean of the transport expenditure in Alaska for the corresponding decade
#(g) “tr_md” – the median of the transport expenditure in Alaska for the corresponding decade.

alaska_summary<-finance_data%>%filter(state=='Alaska')%>%group_by(decade)%>%select(decade,education_expenditure,health_expenditure,transport_expenditure)%>%summarise(across(starts_with(c("education_expenditure","health_expenditure","transport_expenditure")),list(md=median,mn=mean),.names="{substring(.col,1,2)}_{.fn}"))

#Display the Alaska summary data frame
alaska_summary

#A.8 Create a function called impute_by_median which takes as input a vector numerical values, which may include some “NA”s, and replaces any missing values (“NA”s) with the median over the vector.

impute_by_median<-function(x){
med <-median(x,na.rm=1)   # first calculate the median of x
impute_f<-function(z){  # coordinate wise imputation
if(is.na(z)){
return(med)
}  #if z is na replace with mean
else{
return(z)#otherwise leave in place
} 
}
return(map_dbl(x,impute_f)) #apply the map function to impute across vector
}

#generate a subset of your “finance_data” data frame called “idaho_2000” which contains all those rows in which the state column takes the value “Idaho” and the “decade” column takes the value “2000” and includes the columns “year”, “education_expenditure”, “health_expenditure”, “transport_expenditure”, “totals_revenue”,“totals_expenditure”, “totals_savings” (i.e. all columns except “state” and “decade”).


idaho_2000<-finance_data%>%filter(state=='Idaho' & decade==2000)%>%select(-state,-decade)

#Now apply your “impute_by_median” data frame to create a new data frame called “idaho_2000_imputed” which is based on your existing “idaho_2000” data frame but with any missing values replaced with the corresponding median value for the that column. 

idaho_2000_imputed<-idaho_2000%>%select(year,health_expenditure,education_expenditure,totals_savings)%>%summarise(year,across(where(is.numeric),~impute_by_median(.x)))

#Display the imputed values
idaho_2000_imputed


#Section B

#B.1 

#Using the Law of total probability and Baye's theorem -we derive the value of phi returned by the function below
#phi=p1*q/(p1*q+p0*(1-q))

#A)
c_prob_person_given_alarm<-function(p0,p1,q){
  phi=p1*q/(p1*q+p0*(1-q))
  return (phi)
}

#b
#A)
c_prob_person_given_alarm(0.05,0.95,0.1)

# The answer comes out to be 0.6785714
#c
#A)
prob_alarm<-data.frame(q=seq(0.1,1,0.1))%>%
mutate(prob_q=c_prob_person_given_alarm(0.05,0.95,q))

prob_alarm%>%ggplot(aes(x=q, y=prob_q)) + geom_point(shape=1)+geom_smooth(method=lm)


#B2 \end{lcases}

#a) Probability mass function pX : R → [0, 1] 

```

\[ P(x) =\left\{
                \begin{array}{ll}
          1-\alpha-\beta-\gamma\ if\ x=0 \\ 
          \hspace{5 mm}\alpha\  \hspace{15 mm} if\ x = 1 \\ 
          \hspace{5 mm}\beta\ \hspace{15 mm}if\ x=2 \\ 
          \hspace{5 mm}\gamma\ \hspace{15 mm}if\ x=5\ \\ 
           \hspace{5 mm} 0\ \hspace{15 mm} otherwise.
           \end{array}
              \right.
            \]

```{r}
#b) Expectancy of X in terms of α,β,γ 
#A)
```

  
 \[ E[X] =  \alpha+2\beta+5\gamma\  \]
 
```{r}
#c) Expression for population variance of X in terms of α,β,γ 
#A)
```
 
  
 \[ Var[X] =E[X^2]- E[X]^2  \\
  =(\alpha+4\beta+25\gamma) -(\alpha+2\beta+5\gamma)^2 \\
  =\alpha+4\beta+25\gamma- \alpha^2-4\beta^2-25\gamma^2-4\alpha\beta-20\beta\gamma-10\alpha\gamma
 \] 
```{r}
#d) Give an expression for the expectation of the random variable X in terms of α,β,γ.
#A)
```  
 
  
 \[ 
   E(\overline X) = E(\frac{\sum_{i=1}^{n}X_i}n) =\frac{(\alpha+2\beta+5\gamma)\ n}n \\
   =(\alpha+2\beta+5\gamma)\]
   
```{r}
#e) Give an expression for the population variance of the random variable X in terms of α,β,γ,n
#A)
```     
    \[ Var(\overline X) =E[\overline X^2]- E[\overline X]^2  \\
    =E[\frac{\sum_{i=1}^{n}X_i^2}n]- E[\frac{\sum_{i=1}^{n}X_i}n]^2  \\
  =\frac{n(\alpha+4\beta+25\gamma)}{n^2} -\frac{n(\alpha+2\beta+5\gamma)^2}{n^2} \\
  =\frac{\alpha+4\beta+25\gamma- (\alpha^2+4\beta^2+25\gamma^2+4\alpha\beta+20\beta\gamma+10\alpha\gamma)}n
 \] 
```{r}
#f)Create a function called sample_X_0125() which takes as inputs α, β, γ and n and outputs a sample X1, . . . , Xn of independent copies of X where P (X = 1) = α, P (X = 2) = β, P (X = 5)= γ and P (X /∈ {0, 1, 2, 5}) = 0

#A)
sample_X_0125<-function(n,alpha,beta, gamma){
sample_X<-data.frame(U=runif(n))%>%   #run if simulates the uniform distribution
mutate(X=case_when(
(0<=U)&(U<alpha)~1,
(alpha<=U)&(U<alpha+beta)~2,
(alpha+beta<=U)&(U<alpha+beta+gamma)~5,
(alpha+beta+gamma<=U)&(U<=1)~0))%>%
pull(X)
return(sample_X)
} 

#g) Consider α = 0.1, β = 0.2, γ = 0.3,n=100000. Use the function to generate the data
#A)
sample_X<-sample_X_0125(100000,0.1,0.2,0.3)
mean(sample_X)
var(sample_X)
#The value of sample mean comes out to be 2.00205
#The value of the sample variance comes out to be 4.40717

# For question number (d) , the value calculated for the expectation E(X bar) is α+2β+5γ. On replacing the probabilities values in the above result we arrive at the expected value for sample mean as (0.1+2*0.2+5*0.3)=(.1+0.4+1.5)=2 .This the result I was expecting as it is very close to the value of the mean(sample_X) result which is 2.00205
#The is because as per the law of large numbers we expect the sample average to be  close to the expectation for large samples of independent and identically distributed random variables

# For question number (e) , the value calculated for the population  V(X bar) is calculated in terms of α,β, γ and n  On replacing the probabilities values in the above result we arrive at the population variance of X bar as 4.4/100000 =0.000044.This the result I was expecting as it is very close to the calculated value of the  sample variance var(sample_X) =4.407 divided  by the sample size which is 4.4*100000 =0.000044

#(h)
#Once again, take α = 0.1, β = 0.2, γ = 0.3. Conduct a simulation study to explore the behavior of the sample mean. Your study should involve 10000 trials. In each trial, you should set n = 100 and create a sample X1, . . . , Xn of independent and identically distributed random variables with P (Xi = 1) = α,P (Xi = 2) = β, P (Xi = 5) = γ and P (Xi ∈ { / 0, 1, 2, 5}) = 0 for i = 1, . . . , n. For each of the 10000 trials, compute the corresponding sample mean X based on X1, . . . , Xn.


set.seed(0) # set the seed to create reproduciblity
num_trials<-100000
n=100
alpha=0.1
beta=0.2
gamma=0.3

simulate_sample_mean<-crossing(trial=seq(num_trials),sample_size=n)%>%
# create data frame of all trials of the sample size
                     mutate(sample_X=map(.x=trial,~sample_X_0125(n,alpha,beta,gamma)))%>%
# create samples of f independent and identically distributed random variables for each of those trials using sample_X_0125 function
                     mutate(sample_mean=map_dbl(.x=sample_X,.f=mean))
# Compute the sample mean for each of these.

#(i)Generate a histogram plot which displays the behavior of the sample mean within your simulation study.Use a bin width of 0.02. The height of each bar should correspond to the number of times the sample mean took on a value within the corresponding bin

#hist(simulate_sample_mean$sample_mean)

ggplot(simulate_sample_mean, aes(x=sample_mean))+geom_histogram(binwidth=0.02,color="black", fill="white")

#j) numerical values of expectation in the simulation study
    x_mean <-mean(simulate_sample_mean$sample_mean) 
    x_mean
    # comes out to be 2.000972 and 2.0009 up-to four places of decimals
    
    #numerical values of variance in the simulation study
    x_var<-var(simulate_sample_mean$sample_mean) 
    x_var
    # comes out to be 0.04397059 and 0.0439 up-to four places of decimals

#k)  Now append to your histogram plot an additional curve of the form x 7→ 200 · fµ,σ(x), which displays a re scaled version of the probability density function of a Gaussian random variable with population mean µ = E(X) and population variance σ2 = Var(X).  
#A
#Set the parameters of the pdf curve as per mentioned
tail_inc<-seq(x_mean-4*x_var,
x_mean+4*x_var,0.0001) # generate indices
colors<-c("PDF"="red","Histogram"="blue") # set color legend    

ggplot(simulate_sample_mean,aes(x=sample_mean))+geom_histogram(binwidth=0.02,color="blue", fill="white")+geom_line(data=data.frame(tail=tail_inc,Density=10000*dnorm(tail_inc,
mean=x_mean,sd=x_var)),
aes(x=tail,y=Density,color="PDF"))

#(l) Discuss the relationship between the histogram and the additional curve you observe. Can you explain what you observe?
#A : When we see the histogram plot we can see that it follows a normal distribution pattern similar to the new additional curve which is introduced and the tip being around ~2 which is the sample mean for both of them. So, the strongest density of population in both cases is around 2. However the standard deviation or width of the curve is much less in case of the new additional curve that what is observed with respect to the histogram plot. So the data in the histogram plot is more spread out.

#B.3
#a) Give a formula for the the population mean and variance of an exponential random variable X with parameter λ.
# A
```  
    \[ Population Mean =E[X] =\int_{-\infty}^{\infty}x.p_{\lambda}(x)dx  =\int_0^{\infty}x\lambda e^{-\lambda x} dx \hspace{40mm}\\ \\
   = [-xe^{-\lambda x}]_0^{\infty}+\int_0^{\infty}e^{-\lambda x}dx \\
   = [-\frac{1}\lambda e^{-\lambda x}]_0^{\infty}=\frac{1}\lambda
  \\
  Using\hspace{2 mm}Integration\hspace{2 mm} by\hspace{2 mm} parts,
  E[X^2]=\int_{-\infty}^{\infty}x^2.p_{\lambda}(x)dx=\int_0^{\infty}x^2\lambda e^{-\lambda x} dx \hspace{40mm}\\\\
  = [-x^2e^{-\lambda x}]_0^{\infty}+2\int_0^{\infty}xe^{-\lambda x}dx \\
 = \frac{2}\lambda\int_0^{\infty}x\lambda e^{-\lambda x}dx=\frac{2}\lambda E[X]\\
 =\frac{2}{\lambda^2} \hspace{40mm}\\
 Var[X]=E[X^2]-E[X]^2 \hspace{40mm}\\
 =\frac{2}{\lambda^2}-\frac{1}{\lambda^2}=\frac{1}{\lambda^2}\hspace{20mm}\\
   \] 

```{r}
#b)Give a formula for the cumulative distribution function and the quantile function for exponential random variables with parameter λ.
``` 


```{r}
#Section C : 

#The task and method I plan to explore is Linear Regression→Ridge regression which is a form of Supervised Learning. For the method being explored and model being developed:
#1) Root Mean Squared error will be minimized using Ordinary Least squares (OLS) technique,  We will try to choose the parameter which minimizes validation eror.
#2) In ridge regression, we attempt to assign a value for λ (hyper-parameter) that produces the lowest possible test MSE (mean squared error).
  
#Data set to be used
#Housing in London | Kaggle
#Location of the data set
#https://www.kaggle.com/justinas/housing-in-london?select=housing_in_london_monthly_variables.csv  

#Load the data set in R

#Locate the folder path to read the file
folder_path<-"/Users/bikramjitchowdhury/Downloads/SCEM/jh21478_EMATM0061_summative_assessment/jh21478_EMATM0061_C/"
#Define the file name
file_name<-"housing_in_london_monthly_variables.csv" #
#Define the file path
file_path<-paste(folder_path,file_name,sep="") # create the file_path
#Read the file and assign it to a new data frame housing_in_london_monthly 

housing_in_london_monthly_original<-read_csv(file_path) 

#Display the data
housing_in_london_monthly_original

#Rows: 13549 Columns: 7
#Variables in the data set Include : Date , Area, Average_price, Code, houses_sold, No_of_crimes, borough_flag

#Variable Type : date → Continuous,
#Area → Categorical
#average Price→Continuous
#code→Categorical
#houses_sold→ Continuous
#No of crimes→Continuous
#Borough Flag → Binary

#Predictive Model :The aim is to predict the change in average house prices of different areas(boroughs) over time using the mentioned supervised learning algorithm.
#Target Variable (Label) : - Average Price.

#Feature Variable : 1) Area
#2)Date
#3)No of Crimes
#4) Houses sold.
#Performance Metric : Mean Squared error

#Train-Validate- Test Split(%)
#Exploratory Data Analysis :The data set contains 13549 rows with null values on no of crimes and houses sold.
#On dropping those rows the rows remaining will amount to 7375.
#For training – 50% of the data shall be used.
#For Validation – 25% of the data shall be used. – Hyperparameter lambda λ will be tuned in this step
#For Testing – Remaining 25% of the data shall be used.

housing_in_london_monthly<-na.omit(housing_in_london_monthly_original)

housing_in_london_monthly%>%dim()

housing_in_london_monthly<-housing_in_london_monthly%>%select(houses_sold,no_of_crimes,average_price)

str(housing_in_london_monthly)

#construct a plot for the average price to houses sold
#ggplot(housing_in_london_monthly, aes(x=houses_sold, y=average_price)) + geom_point(shape=1)+geom_smooth(method=lm)

#construct a plot for the average price to area
#ggplot(housing_in_london_monthly, aes(x=area, y=average_price)) + geom_point(shape=1)+geom_smooth(method=lm)

#construct a plot for the average price to date
#ggplot(housing_in_london_monthly, aes(x=date, y=average_price)) + geom_point(shape=1)+geom_smooth(method=lm)

#construct a plot for the average price to number of crimes committed

#ggplot(housing_in_london_monthly, aes(x=no_of_crimes, y=average_price)) + geom_point(shape=1)+ geom_smooth(method=lm)

#Create the training, validation and test split
num_total<-housing_in_london_monthly%>%nrow()
num_total
num_train<-floor(0.5*num_total)
num_train
num_validate<-floor(0.25*num_total)
num_validate
num_test<-num_total-num_train-num_validate
num_test

#Randomly slice samples for the test,validation and train data
set.seed(123) #set random seed for reproducibility
test_inds<-sample(seq(num_total),num_test) #test indices
validate_inds<-sample(setdiff(seq(num_total),test_inds),num_validate) #validate inds
train_inds<-setdiff(seq(num_total),union(validate_inds,test_inds))

#Create the train,validation and test data sets based on their indices

housing_train<-housing_in_london_monthly%>%filter(row_number() %in% train_inds) #train data
housing_validate<-housing_in_london_monthly%>%filter(row_number() %in% validate_inds) #validate data
housing_test<-housing_in_london_monthly%>%filter(row_number() %in% test_inds) #test data

#Now split the train,validation and test data sets into feature vectors and labels.

housing_train_x <-housing_train%>%select(-average_price)%>%as.matrix() #train features
housing_train_y <-housing_train%>%pull(average_price) #train labels

housing_validate_x <-housing_validate%>%select(-average_price)%>%as.matrix() #validate features
housing_validate_y <-housing_validate%>%pull(average_price) #validate labels


housing_test_x <-housing_test%>%select(-average_price)%>%as.matrix() #test features
housing_test_y <-housing_test%>%pull(average_price) #test labels

#Now construct a function to train a ridge regression model and check validation performance
compute_train_validate_error_ridge<-function(train_x,train_y,validate_x,validate_y,lambda)
{
  glmRidge=glmnet(x=train_x,y=train_y ,alpha=0,lambda=lambda) #train model
  train_y_est<-predict(glmRidge,newx=train_x) #train predictions
  train_error=mean(train_y-train_y_est^2)
  
  validate_y_est<-predict(glmRidge,newx=validate_x)  #validate predictions
  validate_error=mean((validate_y-validate_y_est)^2) #validation error
  return(list(train_error=train_error,validate_error=validate_error))
  
}

#Now choose a set of hyper-parameters to consider.
lambda_min=0.0001
lambdas=0.0001*(1.1^seq(250))

compute_train_validate_error_ridge(train_x=housing_train_x,train_y=housing_train_y,validate_x=housing_validate_x,validate_y=housing_validate_y,lambda=0.001)

#Now train a ridge regression model for each hyper-parameter and compute validation error

ridge_results_df<-data.frame(lambda=lambdas)%>%mutate(out=map(lambda,~compute_train_validate_error_ridge(train_x=housing_train_x,train_y=housing_train_y,validate_x=housing_validate_x,validate_y=housing_validate_y,lambda=.x)))%>%mutate(train_error=map_dbl(out,~((.x)$train_error)),validate_error=map_dbl(out,~((.x)$validate_error)))%>%select(-out)

#Now find the minimum validation error
min_validation_error<-ridge_results_df%>%pull(validate_error)%>%min()

optimal_lambda<-ridge_results_df%>%filter(validate_error==min_validation_error)%>%pull(lambda)

#Get and display the optimal lambda here
optimal_lambda

#Now extract the ridge regression model with the optimal hyper-parameter
final_ridge_model <-glmnet(x=housing_train_x,y=housing_train_y ,alpha=0,lambda=optimal_lambda)

#Now use the test data to estimate the out-of-sample performance of the trained model

final_ridge_test_y_est<-predict(final_ridge_model,newx=housing_test_x)  #test predictions
final_ridge_test_error=mean((housing_test_y-final_ridge_test_y_est)^2) #test error
```



## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
