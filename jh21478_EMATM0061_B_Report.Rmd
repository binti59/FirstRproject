---
title: "jh21478_EMATM0061_B_Report"
author: "Bikramjit Chowdhury"
date: "09/01/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}

##install.packages("tidyverse")
##install.packages("caret")
library(tidyverse)
##install.packages("Stat2Data")
library(Stat2Data)
library(readxl) #load the readxl library
library(ggplot2)
library(glmnet)
library(stringr)
library(caret)

#Section B

#B.1 

#Using the Law of total probability and Baye's theorem -we derive the value of phi returned by the function below
#phi=p1*q/(p1*q+p0*(1-q))

#a)The function is given below

c_prob_person_given_alarm<-function(p0,p1,q){
  phi=p1*q/(p1*q+p0*(1-q))
  return (phi)
}

#b When  p0 = 0.05, p1 = 0.95 and q = 0.1 , ϕ is

c_prob_person_given_alarm(0.05,0.95,0.1)

# The answer comes out to be 0.6785714

#c Here take ,p0 = 0.05, p1 = 0.95 a

prob_alarm<-data.frame(q=seq(0.1,1,0.1))%>%
mutate(prob_q=c_prob_person_given_alarm(0.05,0.95,q))

#A plot which shows ϕ as we vary q.
prob_alarm%>%ggplot(aes(x=q, y=prob_q)) + geom_point(shape=1)+geom_smooth(method=lm)


#B2 \end{lcases}

#a) Probability mass function pX : R → [0, 1] 

```

\[ P(x) =\left\{
                \begin{array}{ll}
          1-\alpha-\beta-\gamma\ if\ x=0 \\ 
          \hspace{5 mm}\alpha\  \hspace{15 mm} if\ x = 1 \\ 
          \hspace{5 mm}\beta\ \hspace{15 mm}if\ x=2 \\ 
          \hspace{5 mm}\gamma\ \hspace{15 mm}if\ x=5\ \\ 
           \hspace{5 mm} 0\ \hspace{15 mm} otherwise.
           \end{array}
              \right.
            \]

```{r}
#b) Expectancy of X in terms of α,β,γ 
#A)
```
  
 \[ E[X] =  \alpha+2\beta+5\gamma\  \]
 
```{r}
#c) Expression for population variance of X in terms of α,β,γ 
#A)
```
  
 \[ Var[X] =E[X^2]- E[X]^2  \\
  =(\alpha+4\beta+25\gamma) -(\alpha+2\beta+5\gamma)^2 \\
  =\alpha+4\beta+25\gamma- \alpha^2-4\beta^2-25\gamma^2-4\alpha\beta-20\beta\gamma-10\alpha\gamma
 \] 
```{r}
#d) Give an expression for the expectation of the random variable X in terms of α,β,γ.
#A)
```  
 
$…$
 \[ 
   E(\overline X) = E(\frac{\sum_{i=1}^{n}X_i}n) =\frac{(\alpha+2\beta+5\gamma)\ n}n \\
   =(\alpha+2\beta+5\gamma)
   \]
   
```{r}
#e) Give an expression for the population variance of the random variable X in terms of α,β,γ,n
#A)
```     
  $…$
    \[ 
    Var(\overline X) =E[\overline X^2]- E[\overline X]^2  \\
    =E[\frac{\sum_{i=1}^{n}X_i^2}n]- E[\frac{\sum_{i=1}^{n}X_i}n]^2  \\
  =\frac{n(\alpha+4\beta+25\gamma)}{n^2} -\frac{n(\alpha+2\beta+5\gamma)^2}{n^2} \\
  =\frac{\alpha+4\beta+25\gamma- (\alpha^2+4\beta^2+25\gamma^2+4\alpha\beta+20\beta\gamma+10\alpha\gamma)}n
 \] 
```{r}
#f)Create a function called sample_X_0125() which takes as inputs α, β, γ and n and outputs a sample X1, . . . , Xn of independent copies of X where P (X = 1) = α, P (X = 2) = β, P (X = 5)= γ and P (X /∈ {0, 1, 2, 5}) = 0

#A)
sample_X_0125<-function(n,alpha,beta, gamma){
sample_X<-data.frame(U=runif(n))%>%   #run if simulates the uniform distribution
mutate(X=case_when(
(0<=U)&(U<alpha)~1,
(alpha<=U)&(U<alpha+beta)~2,
(alpha+beta<=U)&(U<alpha+beta+gamma)~5,
(alpha+beta+gamma<=U)&(U<=1)~0))%>%
pull(X)
return(sample_X)
} 

#g) Consider α = 0.1, β = 0.2, γ = 0.3,n=100000. Use the function sample_X_0125 to generate the data
#A)
sample_X<-sample_X_0125(100000,0.1,0.2,0.3)
mean(sample_X)
var(sample_X)
#The value of sample mean comes out to be 1.99867
#The value of the sample variance comes out to be 4.395532

# For question number (d) , the value calculated for the expectation E(X bar) is α+2β+5γ. On replacing the probabilities values in the above result we arrive at the expected value for sample mean as (0.1+2*0.2+5*0.3)=(.1+0.4+1.5)=2 .This the result I was expecting as it is very close to the value of the mean(sample_X) result which is 2.00205
#The is because as per the law of large numbers we expect the sample average to be  close to the expectation for large samples of independent and identically distributed random variables

# For question number (e) , the value calculated for the population  V(X bar) is calculated in terms of α,β, γ and n  On replacing the probabilities values in the above result we arrive at the population variance of X bar as 4.4/100000 =0.000044.This the result I was expecting as it is very close to the calculated value of the  sample variance var(sample_X) =4.407 divided  by the sample size which is 4.4*100000 =0.000044

#(h)
#Once again, take α = 0.1, β = 0.2, γ = 0.3. Conduct a simulation study to explore the behavior of the sample mean. Your study should involve 10000 trials. In each trial, you should set n = 100 and create a sample X1, . . . , Xn of independent and identically distributed random variables with P (Xi = 1) = α,P (Xi = 2) = β, P (Xi = 5) = γ and P (Xi ∈ { / 0, 1, 2, 5}) = 0 for i = 1, . . . , n. For each of the 10000 trials, compute the corresponding sample mean X based on X1, . . . , Xn.


set.seed(0) # set the seed to create reproduciblity
num_trials<-100000
n=100
alpha=0.1
beta=0.2
gamma=0.3

simulate_sample_mean<-crossing(trial=seq(num_trials),sample_size=n)%>%
# create data frame of all trials of the sample size
                     mutate(sample_X=map(.x=trial,~sample_X_0125(n,alpha,beta,gamma)))%>%
# create samples of f independent and identically distributed random variables for each of those trials using sample_X_0125 function
                     mutate(sample_mean=map_dbl(.x=sample_X,.f=mean))
# Compute the sample mean for each of these.

#(i)Generate a histogram plot which displays the behavior of the sample mean within your simulation study.Use a bin width of 0.02. The height of each bar should correspond to the number of times the sample mean took on a value within the corresponding bin

#hist(simulate_sample_mean$sample_mean)

ggplot(simulate_sample_mean, aes(x=sample_mean))+geom_histogram(binwidth=0.02,color="black", fill="white")

#j) Numerical values of expectation in the simulation study
    x_mean <-mean(simulate_sample_mean$sample_mean) 
    x_mean
    # comes out to be 2.00102 and 2.0010 up-to four places of decimals
    
    #Numerical values of variance in the simulation study
    x_var<-var(simulate_sample_mean$sample_mean) 
    x_var
    # comes out to be 0.04387057 and 0.0439 up-to four places of decimals

#k)  Now append to the histogram plot an additional curve of the form x 7→ 200 · fµ,σ(x), which displays a re scaled version of the probability density function of a Gaussian random variable with population mean µ = E(X) and population variance σ2 = Var(X).  
#A
#Set the parameters of the pdf curve as per mentioned
tail_inc<-seq(x_mean-4*x_var,
x_mean+4*x_var,0.0001) # generate indices
colors<-c("PDF"="red","Histogram"="blue") # set color legend    

ggplot(simulate_sample_mean,aes(x=sample_mean))+geom_histogram(binwidth=0.02,color="blue", fill="white")+geom_line(data=data.frame(tail=tail_inc,Density=10000*dnorm(tail_inc,
mean=x_mean,sd=x_var)),
aes(x=tail,y=Density,color="PDF"))

#(l) Discuss the relationship between the histogram and the additional curve you observe. Can you explain what you observe?
#A : When we see the histogram plot we can see that it follows a normal distribution pattern similar to the new additional curve which is introduced and the tip being around ~2 which is the sample mean for both of them. So, the strongest density of population in both cases is around 2. However the standard deviation or width of the curve is much less in case of the new additional curve that what is observed with respect to the histogram plot. So the data in the histogram plot is more spread out.

#B.3
#a) Give a formula for the the population mean and variance of an exponential random variable X with parameter λ.
# A
```  
  $…$
    \[ 
    Population Mean =E[X] =\int_{-\infty}^{\infty}x.p_{\lambda}(x)dx  =\int_0^{\infty}x\lambda e^{-\lambda x} dx \hspace{40mm}\\ \\
   = [-xe^{-\lambda x}]_0^{\infty}+\int_0^{\infty}e^{-\lambda x}dx \\
   = [-\frac{1}\lambda e^{-\lambda x}]_0^{\infty}=\frac{1}\lambda
  \\
  Using\hspace{2 mm}Integration\hspace{2 mm} by\hspace{2 mm} parts,
  E[X^2]=\int_{-\infty}^{\infty}x^2.p_{\lambda}(x)dx=\int_0^{\infty}x^2\lambda e^{-\lambda x} dx \hspace{40mm}\\\\
  = [-x^2e^{-\lambda x}]_0^{\infty}+2\int_0^{\infty}xe^{-\lambda x}dx \\
 = \frac{2}\lambda\int_0^{\infty}x\lambda e^{-\lambda x}dx=\frac{2}\lambda E[X]\\
 =\frac{2}{\lambda^2} \hspace{40mm}\\
 Var[X]=E[X^2]-E[X]^2 \hspace{40mm}\\
 =\frac{2}{\lambda^2}-\frac{1}{\lambda^2}=\frac{1}{\lambda^2}\hspace{20mm}\\
   \] 
$…$
```{r}
#b)Give a formula for the cumulative distribution function and the quantile function for exponential random variables with parameter λ.
``` 
  
  $.$ 
 \[ 
   Cumulative\hspace{2mm}distribution\hspace{2mm} function\hspace{2mm} is \hspace{2mm}     given\hspace{2mm}by F_\lambda(x) =\int_{-\infty}^{x}x.p_{\lambda}(t)dt=\left\{
 \begin{array}{ll}0 \hspace{25mm}  if \hspace{2mm}  x<=0 \\ \int_0^{x}x\lambda e^{-\lambda t}dt\hspace{8mm}if\hspace{2mm}x>0
  \end{array}\right.\\\int_0^{x}x\lambda e^{-\lambda t}dt= [-e^{-\lambda t}]_0^{x}=1-e^{-\lambda x}\\Therefore,\hspace{2mm}Cumulitive\hspace{2mm}distribution\hspace{2mm} function\hspace{2mm}=F_\lambda(x) =\int_{-\infty}^{x}x.p_{\lambda}(t)dt=\left\{
 \begin{array}{ll}0 \hspace{25mm}  if \hspace{2mm}  x<=0 \\1-e^{-\lambda x}\hspace{8mm}  if \hspace{2mm}  x>0
  \end{array}\right. \\ \hspace{2mm}Quantile\hspace{2mm}function\hspace{2mm}=inf \{x ∈ R : F_\lambda(x) ≤ p\}\\  F^{-1}_\lambda(p)=\left\{
 \begin{array}{ll}{-\infty} \hspace{20mm}  if \hspace{2mm}  p=0 \\
  \frac{1}{\lambda}{\ln}(1-p)\hspace{8mm}  if \hspace{2mm}  p∈(0,1]
  \end{array}\right.
  \] 
 $…$
```{r}
 #(c)Suppose that X1, · · · , Xn is an i.i.d sample from the exponential distribution with an unknown parameter λ0 > 0. What is the maximum likelihood estimate λˆMLE for λ0?
#A.  
```
 $…$
 \[
   Likelihood\hspace{2mm}\lambda_0\hspace{2mm}given\hspace{2mm}x_1\hspace{2mm}\\
    L(\lambda|x_1)=\lambda e^{-\lambda x_1}\\    Similarly,\\
     L(\lambda|x_2)=\lambda e^{-\lambda x_2}\\     .\\     .\\L(\lambda|x_n)=\lambda e^{-\lambda x_n}\\       \\    Now,\hspace{2mm}Likelihood\hspace{2mm}\lambda_0 \hspace{2mm}given \hspace{2mm} x_1,x_2,...and \hspace{2mm} x_n \hspace{2mm}=\\     =L(\lambda|x1\hspace{2mm}and\hspace{2mm} x2 \hspace{2mm} and \hspace{2mm}...x_n)=     L(\lambda|x_1).L(\lambda|x_2)....L(\lambda|x_n)\\=\lambda e^{-\lambda x_1} .\lambda e^{-\lambda x_2}...\lambda e^{-\lambda x_n} =\lambda^n e^{-\lambda(x_1+x_2+..+x_n)}\\      \\
      Now,\hspace{2mm}Maximum \hspace{2mm}Likelihood\hspace{2mm} is \hspace{2mm}\lambda_{MLE} \hspace{2mm}derived \hspace{2mm} by\hspace{2mm} setting\\ \\ \frac{\partial L(\lambda|x1\hspace{2mm}and\hspace{2mm} x2 \hspace{2mm} and \hspace{2mm}...x_n)}{\partial\lambda} =\frac{\partial \lambda^n e^{-\lambda(x_1+x_2+..+x_n)}} {\partial\lambda} =0\\         \\        Now,\hspace{2mm}Taking \hspace{2mm}derivate\hspace{2mm} of \hspace{2mm}log \hspace{2mm}instead \hspace{2mm} w.r.t \hspace{2mm}\lambda_0\\         \\        \frac{\partial log(\lambda^n e^{-\lambda(x_1+x_2+..+x_n)})} {\partial\lambda}=0\\        \\        =>\frac{\partial log(\lambda^n)}{\partial\lambda}+\frac{\partial log e^{-\lambda(x_1+x_2+..+x_n)}}{\partial\lambda}=0 \\        =>\frac{n}{\lambda}-(x_1+x_2+..+x_n)=0 \\        =>\lambda_{MLE}=\frac{n}{(x_1+x_2+..+x_n)}
     \] 
     
```{r}
# d) Conduct a simulation study to explore the behavior of the maximum likelihood estimator λˆMLE for λ0on simulated data X1, · · · , Xn generated using the exponential distribution. Consider a setting in which λ0 = 0.01 and generate a plot of the mean squared error as a function of the sample size. 

set.seed(0)
num_trials_per_sample_size<-100
min_sample_size<-5
max_sample_size<-1000
sample_size_inc<-5
lambda_0<-0.01

exponential_simulation_df<-crossing(trial=seq(num_trials_per_sample_size),sample_size=seq(min_sample_size,max_sample_size,sample_size_inc))%>%
  # create data frame of all pairs of samples size and trials.
  mutate(simulation=pmap(.l=list(trial,sample_size),.f=~rexp(.y,lambda_0)))%>%
  # simulate sequences of exponential random variables
  mutate(lambda_mle=map_dbl(.x=simulation,.f=~solve(mean(.x))))%>%
# compute the inverse of the mean
group_by(sample_size)%>%
summarise(msq_error=mean((lambda_mle-lambda_0)^2))

#Simulate the root mean squared error for the problem
exponential_simulation_df%>%
ggplot(aes(x=sample_size,y=msq_error))+geom_smooth()+theme_bw()+xlab("Sample size")+ylab("Mean square error")

#Locate the folder path to read the file
folder_path<-"/Users/bikramjitchowdhury/Downloads/SCEM/jh21478_EMATM0061_summative_assessment/jh21478_EMATM0061_B/"  # Give this path as where you would place your file
#Define the file name
file_name<-"bird_data_EMATM0061.csv" #
#Define the file path
file_path<-paste(folder_path,file_name,sep="") # create the file_path
#Read the file and assign it to a new data frame birds_data_original

birds_data_original<-read_csv(file_path) 

birds_data_original<-birds_data_original%>%mutate(time_diffs=lead(Time)-Time)
time_diffs<-birds_data_original%>%pull(time_diffs)
#e) Maximum likelihood estimate of the  rate parameter is give by lambda_MLE
lambda_MLE<-1/mean(time_diffs,na.rm = TRUE)
lambda_MLE
# 0.004982387

#f)confidence interval for λ0

# Here , n=15064

#Let X1, . . . , Xn ∼ fλ0 are independent and identically distributed random variables with exponential distribution and probability density function
```
$…$
\[
  F_{\lambda_0} (x) =\left\{
 \begin{array}{ll}0 \hspace{18mm}  if \hspace{2mm}  x<=0 \\  \lambda_0 e^{-\lambda_0 t}\hspace{8mm}  if \hspace{2mm}  x>0  \end{array}\right.
  \] 
```{r}
#Given α ∈ [0, 1] let zα/2 be the (1 − α/2)-quantile of a standard Gaussian random variable Z ∼ N (0, 1). We can compute an (approximate) confidence interval with a confidence level of (1 − α) × 100% for λ0 as follows:

```
$…$
\[
 L_{\alpha} (X_1,....X_n)=\frac{1}{\overline X}(1-\frac{z_{\alpha/2}}{\sqrt{n}}) \\ U_{\alpha} (X_1,....X_n)=\frac{1}{\overline X}(1+\frac{z_{\alpha/2}}{\sqrt{n}})
  \] 
```{r}
#A #We can create a function to takes the sample of time differences as a vector for the exponentially distributed variables and return the confidence level for the lamda paramter λ0
exp_confidence_interval<-function(sample,confidence_level){
alpha<-1-confidence_level
z_alpha<-qnorm(1-alpha/2)
n<-length(sample)
mn<-mean(sample,na.rm=TRUE)
ci_l<-(1/mn)*(1-z_alpha/sqrt(n))
ci_u<-(1/mn)*(1+z_alpha/sqrt(n))
return(c(ci_l,ci_u))
}

# Using the above function and setting confidence interval to 95%
confidence_level=0.95
sample<-time_diffs
exp_confidence_interval(sample,confidence_level)

#We can the lower and upper levels of confidence interval for λ0 as 0.004902823 and 0.005061950



#References :
#Assignments Problems and solution from EMATM0061_Stat

```


```{r pressure, echo=FALSE}

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
